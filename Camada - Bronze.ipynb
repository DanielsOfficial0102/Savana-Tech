{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4012f6c-2be6-46cd-af30-6187f0923ac4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ü•â Camada Bronze ‚Äì Coleta e Valida√ß√µes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35f0e453-c2ae-4fe4-be8b-2ce800f227ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### üìå Passo 1: Cria√ß√£o do Schema\n",
    "###### Estrutura medalh√£o ent√£o criei 3 Schemas (Bronze/Silver/Gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ea3b1eb1-aa38-4ba9-8e14-06d38d21af3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS daniel_bronze;\n",
    "CREATE SCHEMA IF NOT EXISTS daniel_silver;\n",
    "CREATE SCHEMA IF NOT EXISTS daniel_gold;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "706f1634-16a7-47e0-a2f1-80a50173c951",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### üìå Passo 2: Carregue as vari√°veis\n",
    "###### Carregar vari√°veis de acesso sem expor no c√≥digo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc112522-1733-4abf-bcb4-d513e932f1b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Carregar vari√°veis de acesso sem expor no c√≥digo\n",
    "%run \"./Camada - Configura√ß√£o\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "069f9678-6cb3-40bb-b42f-c058645b75dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### üìå Passo 3: Ingest√£o do Mongodb\n",
    "\n",
    "###### Alternativa do momento para conseguir extrair o Mongodb foi usar Python com o Spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "149accbf-689f-4aa4-880d-1e312557e9ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importa√ß√µes\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "\n",
    "# Conectar no MongoDB usando as vari√°veis carregadas\n",
    "client = pymongo.MongoClient(\n",
    "    f\"mongodb://{mongodb_user}:{mongodb_pwd}@{mongodb_ip}:27017/?authSource=admin\"\n",
    ")\n",
    "db = client[mongodb_db]\n",
    "collection = db[mongodb_collection]\n",
    "\n",
    "# Buscar os dados\n",
    "data = list(collection.find())\n",
    "df_pd = pd.DataFrame(data)\n",
    "\n",
    "# Corrigir _id\n",
    "if \"_id\" in df_pd.columns:\n",
    "    df_pd[\"_id\"] = df_pd[\"_id\"].astype(str)\n",
    "\n",
    "# Converter para Spark\n",
    "df_trasacoes = spark.createDataFrame(df_pd)\n",
    "\n",
    "# (Opcional) Remover a Tabela antiga visando Evitar Conflitos\n",
    "spark.sql(\"DROP TABLE IF EXISTS adb_cliente_savana_prd.daniel_bronze.transacoes\")\n",
    "\n",
    "# Salvar na Bronze\n",
    "df_trasacoes.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"adb_cliente_savana_prd.daniel_bronze.transacoes\")\n",
    "\n",
    "# Visualizar\n",
    "display(df_trasacoes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "271e6381-7f62-47aa-b953-5abbf2e2469b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### üìå Passo 4: Ingest√£o do SQL\n",
    "\n",
    "###### Com problemas de erros estou fazendo o mesmo processo do Mongodb, ultilizando o Python com o Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31363a6e-fe57-4684-891a-6437380cfcdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importa√ß√µes\n",
    "import pymysql\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import to_date, col\n",
    "\n",
    "# Conectar ao MySQL usando as vari√°veis\n",
    "conn = pymysql.connect(\n",
    "    host=sql_ip,\n",
    "    user=sql_user,\n",
    "    password=sql_pwd,\n",
    "    database=sql_database,\n",
    "    port=3306\n",
    ")\n",
    "\n",
    "# Leitura com Pandas\n",
    "df_pd = pd.read_sql(f\"SELECT * FROM {sql_table}\", conn)\n",
    "\n",
    "# Convers√£o para Spark\n",
    "df_clientes = spark.createDataFrame(df_pd)\n",
    "\n",
    "# (Opcional) Remover a Tabela antiga visando Evitar Conflitos\n",
    "spark.sql(\"DROP TABLE IF EXISTS adb_cliente_savana_prd.daniel_bronze.clientes\")\n",
    "\n",
    "# Salvar na Bronze\n",
    "df_clientes.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"adb_cliente_savana_prd.daniel_bronze.clientes\")\n",
    "\n",
    "# Visualizar\n",
    "display(df_clientes)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7011759843714577,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Camada - Bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
