{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "127f0f98-c799-4d2f-bdee-9100d200429a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "###üß™ Camada Silver ‚Äì Tratamento e Enriquecimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1363062-a787-4daf-9e7e-6b7aa0d8ca0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importa√ß√£o\n",
    "from pyspark.sql.functions import trim, col, when, lit\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Leitura das tabelas bronze\n",
    "df_transacoes = spark.table(\"adb_cliente_savana_prd.daniel_bronze.transacoes\")\n",
    "df_clientes = spark.table(\"adb_cliente_savana_prd.daniel_bronze.clientes\")\n",
    "\n",
    "# Renomear coluna ID para ID_Cliente (para join)\n",
    "df_clientes = df_clientes.withColumnRenamed(\"ID\", \"ID_Cliente\")\n",
    "\n",
    "# Valida√ß√£o antes de fazer a Jun√ß√£o (Join)\n",
    "df_clientes = df_clientes.withColumn(\"ID_Cliente\", col(\"ID_Cliente\").cast(\"long\"))\n",
    "df_transacoes = df_transacoes.withColumn(\"ID_Cliente\", col(\"ID_Cliente\").cast(\"long\"))\n",
    "\n",
    "# Join\n",
    "df_silver = df_transacoes.join(df_clientes, on=\"ID_Cliente\", how=\"left\")\n",
    "\n",
    "# Limpeza de espa√ßos extras\n",
    "for campo in [\"Nome\", \"Cidade\", \"Meio_de_Pgmt\"]:\n",
    "    if campo in df_silver.columns:\n",
    "        df_silver = df_silver.withColumn(campo, trim(col(campo)))\n",
    "\n",
    "# Corre√ß√µes manuais de erros de Nome conhecidos\n",
    "df_silver = df_silver.withColumn(\"Nome\",\n",
    "    when(col(\"Nome\") == \"Ana\", \"Ana Costa\")\n",
    "    .when(col(\"Nome\") == \"Carlos\", \"Carlos Pereira\")\n",
    "    .when(col(\"Nome\") == \"Fernanda\", \"Fernanda Gomes\")\n",
    "    .when(col(\"Nome\") == \"Gabriel\", \"Gabriel Martins\")\n",
    "    .when(col(\"Nome\") == \"Jos\", \"Jos√© da Silva\")\n",
    "    .when(col(\"Nome\") == \"Jos da Silva\", \"Jos√© da Silva\")\n",
    "    .when(col(\"Nome\") == \"Maria\", \"Maria Oliveira\")\n",
    "    .when(col(\"Nome\") == \"Mariana\", \"Mariana Almeida\")\n",
    "    .when(col(\"Nome\") == \"Ricardo\", \"Ricardo Lima\")\n",
    "    .when(col(\"Nome\") == \"Roberto\", \"Roberto Santos\")\n",
    "    .when(col(\"Nome\") == \"Tatiane\", \"Tatiane Ramos\")\n",
    "    .otherwise(col(\"Nome\"))\n",
    ")\n",
    "\n",
    "df_silver = df_silver.withColumn(\"Cidade\",\n",
    "    when(col(\"Cidade\") == \"So Paulo\", \"S√£o Paulo\")\n",
    "    .when(col(\"Cidade\") == \"Bralia\", \"Bras√≠lia\")\n",
    "    .when(col(\"Cidade\") == \"Braslia\", \"Bras√≠lia\")\n",
    "    .when(col(\"Cidade\") == \"Goinia\", \"Goi√¢nia\")\n",
    "    .when(col(\"Cidade\") == \"Joo Pessoa\", \"Jo√£o Pessoa\")\n",
    "    .otherwise(col(\"Cidade\"))\n",
    ")\n",
    "\n",
    "def padronizar_nulls(df):\n",
    "    for campo in df.schema.names:\n",
    "        if df.schema[campo].dataType == StringType():\n",
    "            df = df.withColumn(campo, when(trim(col(campo)) == \"\", None).otherwise(col(campo)))\n",
    "    return df\n",
    "\n",
    "# Aplicar ao df_silver\n",
    "df_silver = padronizar_nulls(df_silver)\n",
    "\n",
    "# (Opcional) Remover a Tabela antiga visando Evitar Conflitos\n",
    "spark.sql(\"DROP TABLE IF EXISTS adb_cliente_savana_prd.daniel_silver.BaseCliente_Transacoes\")\n",
    "\n",
    "# Salvar na camada Silver\n",
    "df_silver.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"adb_cliente_savana_prd.daniel_silver.BaseClientes_Transacoes\")\n",
    "\n",
    "# Exibir\n",
    "display(df_silver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ca83e7f-22a8-4cb3-aff8-5ec5c95ad7fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Teste de Otica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "845b516b-7206-462c-88ef-70ef4fafe6b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import trim, col, when, lit\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Leitura das tabelas bronze\n",
    "df_transacoes = spark.table(\"adb_cliente_savana_prd.daniel_bronze.transacoes\").alias(\"transacoes\")\n",
    "df_clientes = spark.table(\"adb_cliente_savana_prd.daniel_bronze.clientes\").alias(\"clientes\")\n",
    "\n",
    "# Garantir que o tipo de chave seja compat√≠vel\n",
    "df_transacoes = df_transacoes.withColumn(\"ID_Cliente\", col(\"ID_Cliente\").cast(\"long\"))\n",
    "df_clientes = df_clientes.withColumn(\"ID\", col(\"ID\").cast(\"long\"))\n",
    "\n",
    "# Join no estilo do criador do desafio\n",
    "df_silver = df_transacoes.join(\n",
    "    df_clientes,\n",
    "    df_transacoes[\"ID_Cliente\"] == df_clientes[\"ID\"],\n",
    "    \"left\"\n",
    ").drop(\"ID\")\n",
    "\n",
    "# Limpeza de espa√ßos extras\n",
    "for campo in [\"Nome\", \"Cidade\", \"Meio_de_Pgmt\"]:\n",
    "    if campo in df_silver.columns:\n",
    "        df_silver = df_silver.withColumn(campo, trim(col(campo)))\n",
    "\n",
    "# Corre√ß√µes manuais de erros conhecidos\n",
    "df_silver = df_silver.withColumn(\"Nome\",\n",
    "    when(col(\"Nome\") == \"Ana\", \"Ana Costa\")\n",
    "    .when(col(\"Nome\") == \"Carlos\", \"Carlos Pereira\")\n",
    "    .when(col(\"Nome\") == \"Fernanda\", \"Fernanda Gomes\")\n",
    "    .when(col(\"Nome\") == \"Gabriel\", \"Gabriel Martins\")\n",
    "    .when(col(\"Nome\") == \"Jos\", \"Jos√© da Silva\")\n",
    "    .when(col(\"Nome\") == \"Jos da Silva\", \"Jos√© da Silva\")\n",
    "    .when(col(\"Nome\") == \"Maria\", \"Maria Oliveira\")\n",
    "    .when(col(\"Nome\") == \"Mariana\", \"Mariana Almeida\")\n",
    "    .when(col(\"Nome\") == \"Ricardo\", \"Ricardo Lima\")\n",
    "    .when(col(\"Nome\") == \"Roberto\", \"Roberto Santos\")\n",
    "    .when(col(\"Nome\") == \"Tatiane\", \"Tatiane Ramos\")\n",
    "    .otherwise(col(\"Nome\"))\n",
    ")\n",
    "\n",
    "df_silver = df_silver.withColumn(\"Cidade\",\n",
    "    when(col(\"Cidade\") == \"So Paulo\", \"S√£o Paulo\")\n",
    "    .when(col(\"Cidade\") == \"Bralia\", \"Bras√≠lia\")\n",
    "    .when(col(\"Cidade\") == \"Braslia\", \"Bras√≠lia\")\n",
    "    .when(col(\"Cidade\") == \"Goinia\", \"Goi√¢nia\")\n",
    "    .when(col(\"Cidade\") == \"Joo Pessoa\", \"Jo√£o Pessoa\")\n",
    "    .otherwise(col(\"Cidade\"))\n",
    ")\n",
    "\n",
    "# Padronizar campos nulos\n",
    "def padronizar_nulls(df):\n",
    "    for campo in df.schema.names:\n",
    "        if df.schema[campo].dataType == StringType():\n",
    "            df = df.withColumn(campo, when(trim(col(campo)) == \"\", None).otherwise(col(campo)))\n",
    "    return df\n",
    "\n",
    "df_silver = padronizar_nulls(df_silver)\n",
    "\n",
    "# (Opcional) Remover tabela anterior\n",
    "spark.sql(\"DROP TABLE IF EXISTS adb_cliente_savana_prd.daniel_silver.BaseClientes_Transacoes\")\n",
    "\n",
    "# Salvar na camada Silver\n",
    "df_silver.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"adb_cliente_savana_prd.daniel_silver.BaseClientes_Transacoes\")\n",
    "\n",
    "# Verifica√ß√£o de datas v√°lidas\n",
    "from pyspark.sql.functions import count, when\n",
    "\n",
    "df_silver.select(\n",
    "    count(when(col(\"dt_transacao\").isNull(), 1)).alias(\"Nulls\"),\n",
    "    count(when(col(\"dt_transacao\").isNotNull(), 1)).alias(\"Datas v√°lidas\")\n",
    ").show()\n",
    "\n",
    "# Visualizar dados\n",
    "display(df_silver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33871bb2-7214-4ca3-b1b8-708cf5022111",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_mongo = spark.table(\"adb_cliente_savana_prd.daniel_bronze.transacoes\").alias(\n",
    "    \"transacoes\"\n",
    ")\n",
    "\n",
    "df_clientes = spark.table(\"adb_cliente_savana_prd.daniel_bronze.transacoes\").alias(\"clientes\")\n",
    "\n",
    "df_transacoes_clientes = df_transacoes.join(\n",
    "    df_clientes, [col(\"transacoes.ID_Cliente\") == col(\"clientes.ID\")]\n",
    ").drop(\"ID\")\n",
    "\n",
    "df_transacoes_clientes.write.mode(\"overwrite\").option('overwriteSchema', 'true').saveAsTable(\n",
    "    \"adb_cliente_savana_prd.daniel_silver.BaseClientes_Transacoes2\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Camada - Silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
